\section{Introduction}
\label{sec:introduction_dgtta}

    Medical image analysis, particularly medical image segmentation, has made a significant leap forward in recent years with deep learning. However, changes in data distribution introduced by different input modalities or devices can lead to errors in the performance of deep learning models \cite{karani2018lifelong}. Since multiple imaging techniques are often required for disease identification and treatment planning, and MRI devices especially offer broad flexibility in adjusting acquisition parameters, access to all imaging domains is usually infeasible. Consequently, trained models may produce inaccurate results when encountering unseen, out-of-domain data (OOD) at test-time  \cite{pooch2020can}.

    Supervised finetuning can be used as a workaround to retrain or finetune networks for the unseen domain. Still, it would, in turn, require curating and labelling data again, which is costly and time-consuming. The most frequently studied approach to overcome this effort is using unsupervised domain adaptation (UDA), which incorporates labeled source and unlabeled target data \cite{guan2021domain,zhu2017unpaired,zhu2017unpaired,park2020contrastive,choi2021using,chen2020unsupervised,varsavsky2020test}. %TODO add: orbes2019multi, orbes2022augmentation
    Accessing the source and target data jointly in UDA imposes a challenge since source data can be unavailable when privacy concerns or other regulations restrict its transfer. For these reasons, source-free domain adaptation (SFDA) accesses source and target data separately in successive steps  \cite{wen2023source,chen2021source,yang2022source,tarvainen2017mean,wen2023source}.
    Here, methods require retraining on data of the target distribution to adapt models to the target domain. However, the assumption that access to a broad target distribution is available is not always valid, and in addition to that, technical expertise is needed to retrain a model. In practice, a single out-of-domain data sample is usually given for which we want to obtain optimal results immediately. As this is the most realistic and practical scenario, we target it in this study, facing the most challenging data constraints. Two complementary paradigms exist to maximize the performance in such a limited case â€” domain generalization (DG), which optimizes the model performance for `any' unseen out-of-distribution sample and test-time adaptation (TTA), which optimizes the model performance only for the sample in question. DG methods use source-only data and optimize the training routine to derive a powerful base model that is presumably stable even under input distribution shifts \cite{liu2023clip,devries2017improved,xu2020robust,ouyang2022causality,zhou2020deep,hu2022domain,tobin2017domain,billot2023synthseg,bucci2021self,zhou2021models,he2022masked,hoyer2023mic}. For these methods, the challenge arises in foreseeing the possible target domains.
    DG is an ultimate goal to achieve, but up to now, no universal solution that robustly works has been found. TTA, on the other hand, can be used to recover models that perform poorly while only accessing an unlabeled target image, as shown in many studies \cite{wang2020tent,sun2020test,bateson2020source,huang2022online,karani2021test,lyu2022learning,he2020self,he2021autoencoder,liu2022single}.
    We argue that linking both approaches enables optimal separate use of source and target data where DG maximizes the base performance and TTA can further optimize the result. Numerous DG and TTA methods have been developed, but linking them together requires compatibility with the selected methods. This implies that requirements for specific models, architectural changes and complex training routines are unfavourable. Especially, adaptation approaches often require complex strategies and assumptions such as intertwined adaptation layers \cite{he2020self, he2021autoencoder}, indirect supervision tasks \cite{li2022self,huang2022online,lyu2022learning}, prior knowledge about label distributions \cite{bateson2020source} or assumptions on the distinctiveness of domains \cite{varsavsky2020test}, which we also summarize in Sec. \ref{sec:related_work}. % TODO add: orbes2019multi, orbes2022augmentation
    Moreover, when we aim to generate a universal solution to segment medical images in out-of-domain settings, the flexible applicability of the method is essential. This applicability has been overlooked in previous studies, where methods were mostly applied in complex, scattered and separated frameworks. This is why we propose to employ DG-TTA, a minimally invasive approach integrated into the state-of-the-art nnUNet \cite{isensee2021nnu} framework in the next section.


\subsection{Contribution}
    In this study, we consider out-of-domain segmentation across domains with minimum constraints on data availability: Given the source data only at training-time and only one target sample of an unknown domain at test-time later, we aim to achieve an optimal segmentation performance. This crucially enables out-of-the-box use of off-the-shelf pre-trained segmentation networks without further training and could be directly employed in clinical practice.

    \begin{itemize}
        \item We propose combining DG pre-training and TTA to achieve optimal performance with minimum data requirements (\emph{DG-TTA}).
        % \item During DG pre-training solely input modifications are applied not requiring any specific domain knowledge
        \item We introduce using the \emph{MIND} feature descriptor previously only applied in image registration tasks for DG pre-training and show superiority for small-scale datasets.
        \item We perform TTA with a lean self-supervision scheme previously seen in UDA/SFDA \cite{varsavsky2020test,perone2019unsupervised} %TODO: add orbes2019multi, orbes2022augmentation
        but new in this context. We further avoid any other auxiliary optimization tasks and the need for prior assumptions.
        % \item We extensively test our method on models pre-trained with and without DG techniques to show that our method can be applied for a wide range of models.
        \item We show that high-quality predictions of medical images can be obtained out-of-the-box by combining our methodologies, the state-of-the-art nnUNet segmentation framework \cite{isensee2021nnu}, and pre-trained TotalSegmentator CT models.
    \end{itemize}


    % Other notes
    % - Needs source data which makes reusage of models hard -> SFDA
    % SFDA survey (arxiv) \cite{yu2023comprehensive}
    % SF-UDA survey (arxiv) \cite{fang2022source}

    % % SFDA \cite{yang2021exploiting}
    % C-SFDA \cite{karim2023c}


    % domain alignment

    % Contrastive learning  \cite{chaitanya2020contrastive} \cite{chopra2005learning}
    % Adversarial learning \cite{sun2020adversarial}
    % \cite{ganin2016domain}


    % instead of forgetting we enforce further learning through targeted augmentation (see \cite{wen2023source} which only update the encoder to not loose info)




    % In Domain adaptation task is the same but data distribution changes.  \cite{guan2021domain}
    % Label scarcity is a driver for DA in medical imaging \cite{guan2021domain}

    % We  train in single source domain \cite{guan2021domain}
    % Build a common dataset from source and target samples with projecting them into feature space, perform alignment and then train classifier \cite{li2018detecting} (unsupervised shallow DA).


    % leverage multiple source domains and perform transfer learning (shallow)\cite{cheng2017multi}

    % oct, ct mr, cardiac imaging, only first layers, low-level information is updated in the network\cite{yang2020unsupervised}

    % adversarial training of domains, gradient reversal layer (DANN), unsupervised deep DA \cite{ganin2016domain}.

    % \IEEEPARstart{T}{his} document is a template for \LaTeX.
    % You are encouraged to use it to prepare your manuscript.
    % If you are reading a paper or PDF version of this document, please download the
    % \LaTeX .zip file from the IEEE Web site at \underline
    % {https://www.embs.org/tmi/authors-instructions/} to prepare your manuscript.
    % You can also explore using the Overleaf editor at
    % \underline
    % {https://www.overleaf.com/blog/278-how-to-use-overleaf-with-}\discretionary{}{}{}\underline
    % {ieee-collabratec-your-quick-guide-to-getting-started\#.}\discretionary{}{}{}\underline{xsVp6tpPkrKM9}

\section{Related work}
    % Selection based on \url{https://github.com/zhaoxin94/awesome-domain-adaptation}
    \label{sec:related_work}
    \subsection{Unsupervised- and source-free domain adaptation} % Access to source, access to unlabeled targets
    Unsupervised domain adaptation provides the benefit that no labels of the target domain must be available to adapt a model. Methods have been developed to translate input images for optimal fit for the trained model, such as generative adversarial approaches: In \emph{CycleGAN}  \cite{zhu2017unpaired}, unpaired image-to-image translation is performed using two mapping functions to map an image to another domain and vice-versa supervised by discriminator-based- and cycle- consistency losses. Later, \emph{CUT} and \emph{FastCUT} \cite{park2020contrastive}  were developed, optimizing an encoder-decoder structure to extract domain-invariant features and add domain-specific information in the decoding step.  % TODO add \cite{choi2021using}. % encoder decoder structure where the encoder learns domain-invariant features utilizing mutual information of patches and a decoder learns adding domain specific features to the encoded input again.
    % later than cycleGAN CUT / FASTCUT
    % This approach was e.g. applied to the CrossMoDa challenge task \cite{dorent2023crossmoda}, where contrast-enhanced T1 weighted images were translated to high-resolution T2 weighted MRI
    % and similarly in \cite{chen2020unsupervised} where CT and MRI domains were bridged. %TODO readd again
     % Artificially styled and real images of the target domain were processed by an synergistic encoder. To this encoder an image decoder regenerating a source-styled image and a segmentation head was attached. The features of the resulting output branches were then trained with two additional discriminator-based losses aiming to align the segmentation and image space features.
    %SFDA:  No access to source, access to the "complete" unlabeled targets distribution
    Adversely, in \cite{varsavsky2020test}, %TODO add: orbes2019multi, orbes2022augmentation
    the segmentation network is forced to produce domain-invariant features by reversing the gradients of a domain classifier. At the same time, the decision boundary of class predictions is optimized using a consistency soft Dice loss of augmented input samples. The authors present UDA approaches that work with multiple and even on a single target sample. % Test-time Unsupervised Domain Adaptation: Adversarial training (inverted grads for domain classification) and consistency training (also soft-dice loss) combined.
    % \cite{orbes2022augmentation} % Augmentation based unsupervised domain adaptation: Same as ours, but domain discriminator, brain task only,
    % To perform the domain translation source and target image data needs to be available at the same time.

    To further reduce the need to share source data, UDA was sought to be replaced by SFDA methods \cite{yang2022source,wen2023source,chen2021source}.
    Here, a common approach is to leverage pseudo-labels generated by the non-adapted base model and refine them to supervise the model adaptation. In \cite{chen2021source}, estimated class centroids were used as a proxy, and pixel-level uncertainties were used to discard inaccuracies of pseudo labels.
    % % \cite{chen2021source} SFDA. Use class centroids and the distances of individually classified pixels to discard fg pixels if the background centroid is nearer to the pixel than the fg centroid -> this gives pseudo labels with which a target model is supervised.
    More complex proxy pseudo-supervision schemes exist, such as in \cite{yang2022source}, where three cascaded base model variants with differing fixed weights were combined.
    % I.e. three segmentation networks are used in a cascaded manner in \cite{yang2022source} and generate pseudo labels for each other.
    % A first network only shares deeper layers with the source model whereas the first layers are adapted during training.
    % A second network shares all parameters of the source model, is freezed and receives stylized images from a stylization network.
    % The third model's parameters are all learnable and shall output the final segmentation for the target domain.
    % The authors introduce a scheduled iterative and cyclic learning procedure to train the networks.
    % Their method is validated with an abdominal segmentation task where the domain gap of CT to MR images is bridged.
    % \cite{xie2022learn}: Learn Complementary Pseudo-label for Source-free Domain Adaptive Medical Segmentation
    %TODO add number of references to figure
    Another versatile approach for SFDA is the mean teacher paradigm \cite{tarvainen2017mean}, initially developed for classification scenarios where a moving average of the student modelâ€™s weights is used to update a teacher model. In \cite{wen2023source}, a consistency loss between a teacher and student network is used for target data to which flipping and random-noise perturbations were applied. To explicitly update weights related to the domain shift, the distance of the updated weights to the original weights is measured, and a predetermined percentage of the lowest weights, which might not correspond to the domain shift, is not updated. Not constrained to SFDA, mean teacher schemes can also be found in UDA scenarios \cite{perone2019unsupervised}.
    The mentioned approaches in this section have in common that they set strong assumptions on data availability at the time of application --- source data in the case of UDA and a broad spectrum of the target data distribution.
    % (Channel-wise Weight Restoration, CWR). The mean-teacher update is performed in an iterative manner, where first the encoder is updated with CWR, followed by a normal mean-teacher update, followed by a CWR update of the decoder, followed by another normal MT update.
    % WEAKLY-SUPEVERISED DA! The authors of \cite{wang2022metateacher} adopted this paradigm and use multiple teacher models --- each pre-trained on one source domain.
    % This is done in two steps: First, a coordinating weight for the teacher models is learnt with a small portion of labeled data. This weight determines the contribution of each teacher to the target prediction.
    % Then in a subsequent bilevel optimization stage the student's and teachers' weights are updated in a meta-learning manner using labeled pairs as well as unlabeled images.
    % This paper is UDA, not TTA
    % The idea of \cite{hu2021fully} is to align features of the test image % Fully Test-Time Adaptation for Image Segmentation: nuclear-norm maximization and contour regularization: Batch norms only, expensive local nuclear-norm needs maxpooling -> lowered resolution

    \subsection{Domain generalization} % access to source training
    DG techniques are applied during training on the source data and enhance the modelâ€™s out-of-domain performance. One way to improve generalization is to increase the data manifold, such as in \cite{liu2023clip}, where 14 publicly available CT datasets were combined in training. Also, the explicit declaration of domains is possible: Multiple labeled source domains are used during training, incorporating a domain classifier that differentiates between the domains in  \cite{hu2022domain}.
    % The domain classification is then used to condition the weights of the segmentation head. At test-time the domain of an input images is then first classified as a mixture of domains and the conditioned segmentation head is adapted accordingly.
    The training of generalizing models has also been studied with limited, single-source data. This removes the burden of extensive data acquisition. However, model generalization then needs to be enforced otherwise, e.g., through artificially increasing the data by augmentation:
    % In case of image segmentation, intensity-based or spatial augmentations can be applied.
    Augmentations can comprise simple intensity-based modifications such as the application of random noise \cite{zhou2022domain}, partial corruption of image areas such as in \cite{devries2017improved,he2022masked,hoyer2023mic},
    % where rectangular areas of the input image where randomly removed.
    or more sophisticated methods such as convolution-operations with randomly initialized weights  \cite{xu2020robust,ouyang2022causality}.
    Unlike handcrafted or random augmentations, differentiable augmentation schemes were used in adversarial settings, where the applied augmentation was optimized so that the trained network was encouraged to perform well under optimally difficult conditions \cite{zhou2020deep, hu2022domain}.
    Domain randomization \cite{tobin2017domain} leverages a complete virtual simulation of input data to provide broadly varying data, removing the constraints on augmenting captured data samples, and was also successfully applied for medical image segmentation in \cite{billot2023synthseg}.
    % a domain randomization approach was introduced to generate artificial images samples from input segmentations during training.
    DG can not only be achieved by increasing the data manifold but also by adapting training strategies: In \cite{bucci2021self, zhou2021models}, multiple tasks comprising image restoration, classification, and segmentation were used to form a more general understanding of the modelâ€™s underlying data correlations.
    % Multiple samples in memory bank..
    % \emph{DLTTA} \cite{yang2022dltta} first trains a segmentation network. At test-time a fixed-size feature memory bank is used which store the last K features of previous samples. The memory bank is used to measure the divergence to the current batch predicted features. Based on the divergence the learning rate for each sample is adapted dynamically when training it against a self-supervised loss function.

    \subsection{Test-time adaptation} % access to only one sample
    \columnfigure[!t]{\dgttaPath/figures/related_work.pdf}{1.}{Core ideas of related work for TTA in medical image segmentation. \legendbox{hi_2}-colored components are optimized at training-time, belong to the training-routine or knowledge of the training data. \legendbox{hi_4}-colored components are optimized in a consecutive step on the training data. \legendbox{hi_1}-colored components are optimized at test-time through $\mathcal{L}_{TTA}$ loss terms. Shaded \legendbox{hi_2}\/\legendbox{hi_1}-components are optimized during training and test-time adaptation. For a complete overview, please refer to the related works described in the text.}{fig:related_work}

    TTA operates in the target data domain and, as opposed to UDA and SFDA methods, only requires one single target sample for adaptation\footnote{Some authors also use SFDA/UDA and TTA terms interchangeably --- by our definition TTA methods access only a single target sample and not a broader spectrum of the target data distribution.}.
    Model adaptation can be driven by minimizing entropy-based measures \cite{wang2020tent,bateson2020source}
    % \cite{wang2020tent} % TENT: FULLY TEST-TIME ADAPTATION BY ENTROPY MINIMIZATION: Minimize shannon entropy. Process batched data. Update affine transformation weights of channels (lightweight)
    and the introduction of auxiliary tasks \cite{sun2020test,karani2021test,lyu2022learning}.
    % \cite{sun2020test} % Test-time training with self-supervision for generalization under distribution shifts: Add an auxiliary task of rotation and predict rotation angle by shared feature layers.
    % \cite{boudiaf2022parameter} % THIS IS SFDA... Parameter-free Online Test-time Adaptation: Minimize KL term to stay close to initial base model prediction and at the same time enforce features with high affinity to reduce loss by laplacian adjusted maximum likelihood estimation. classification task
    These concepts were applied multifaceted in medical imaging, as depicted in some selected works in Fig. \ref{fig:related_work}.

    % These concepts were applied multifaceted in medical imaging as described in the following:
    % Entropy based + prior
    In \emph{AdaMI} \cite{bateson2020source}, TTA is performed using a class-ratio prior and a mutual information-based entropy minimization of the predicted softmax output maps.
    % complex supervision schemes
    More complex supervision schemes can be found in \emph{RefSeg} \cite{huang2022online}, where a framework iteratively improves predictions by synthesizing input images from edge-maps and segmentation heatmaps. The synthesized images are supervised using normalized cross-correlation and MI-based loss functions.
    % A pre-trained GAN model is used for image synthesization.
    A denoising autoencoder is used in \cite{karani2021test} to fix implausible segmentation output by the base segmentation network. An image normalization layer is then adapted to provide correctly normalized input for the segmentation model by supervising it with the denoised pseudo-label.
    % Test-time adaptable neural networks for robust medical image segmentation: Use denoising autoencoder to pull prediction network to better resemble the true segmentation and update the base network weights to reproduce this prediction.
    % Learning From Synthetic CT Images via Test-Time Training for Liver Tumor Segmentation.
    In \emph{SR-TTT}, a two-task training routine is employed \cite{lyu2022learning}: Healthy images are generated from hand-crafted, synthesized liver tumor images. A subsequent segmentation network predicts tumor masks and a third reconstruction network uses masks and healthy images to synthesize tumor images, forming a closed loop for TTA.
    % where generation models are fixed and only the segmentation model is optimized.
    % The hand-crafted sythesization is omitted during TTA.
    % During TTA a test-time tumor image is inserted into the loop. Generation models are fixed and only the segmentation model is optimized for optimal reconstruction.
    % The method is evaluated on two liver tumor datasets.
    % Feature alignment
    % IIT...
    % Disentangled feature spaces are learnt in the CALAMITI approach in \cite{zuo2021unsupervised} by a strategy where parts of the networks are forced to learn shape information of T1/T2 weighted data (common space) and separated image intensity variation between T1/T2 maps for one acquistition site A. At test-time the same process is then repeated for newly acquired images from a new site B. Thus shape and intensity space are disentangled, the learnt shape model from the source data is combined with a newly learnt intensity model of the target data, where a reconstruction task scenario is used. The authors use their approach for image-to-image translation tasks where a cross-site domain gap of brain scan images is bridged. The authors also evaluate the quality of translated images by applying a segmentation post-processing method and rate the consistency of segmentations.
    % Autoencoder based losses
    Instead of entropy-based losses \emph{SDA-Net} \cite{he2020self, he2021autoencoder} uses several autoencoder-based losses to measure the feature discrepancy of target samples. The architecture consists of a U-Net with specific intertwined layers for adaptation.
    % Additional layers are integrated inbetween convolution blocks in all levels of the U-Net in the encoder and decoder. Additionally, several autoencoders are added to detect and rate feature outliers.
    % For that, encoder and decoder features on all levels of the U-Net were concatenated and the autoencoders trained to reconstruct the features after the training of the base U-Net is finished.
    % During TTA, the adaptor and task specific layers are optimized by measuring the reconstruction error of the autoencoders. The method is validated for OCT inter-scanner segmentation and MRI T1 to T2 synthesis on brain scans.
    % In \cite{li2022self} a similar architecture is proposed: A two-task segmentation and image reconstruction network is first trained with ground truth data. Two autoencoders are trained afterwards to reconstruct segmentation and images separately. During test-time, only a target-to-source image translation network and the encoder of the segmentation network is trained. This is done in via self supervision, where the trained autoencoders are used to measure the segmentation and image input and output discrepancy. The method is tested with a domain gap between 3D adult brain scans and pediatric brain scans. Compared to  \emph{SDA-Net}, the autoencoders are not measuring the main-network feature similarities, but the similarities of the final outputs. Moreover, reconstruction and segmentation tasks were processed jointly.
    % Consistency is used
    Apart from the indirect supervision of pseudo-targets, consistency schemes are also successfully applied: In \cite{liu2022single}, coefficients for a linear combination of fixed shape dictionary items are learned. During TTA, these coefficients are predicted for noise-perturbed input samples, and the consistency of shape coefficients and final segmentations is measured.
    % Single-domain Generalization in Medical Image Segmentation via Test-time Adaptation from Shape Dictionary: While training coefficients for shape combinations are learnt by a regression branch. During TTA noise-pertubed inputs are fed to the trained networks and consistency of shape coefficients is enforced.
    % A multi-source trained Domain and Content Adaptive Convolution (\emph{DCAC}) model is used in \cite{hu2022domain}.
    % The authors hypothesis is that an unseen test-time sample is a mixture of domain samples that have been presented during the multi-source training routine. Alongside the U-Net based segmentation prediction a domain predictor is attached to the encoder feature maps of the network and the one-hot encoded domain prediction is optimized. The distribution vector is then used during TTA to predict the input image domain distribution. Based on the distribution vector MLPs infer the weights of a domain-controller head attached to the U-Net. Similarily the weights of a subsequent content-controller are conditioned using the U-Net's latent features.


    % - approaches are too complex -> here we are


    % DG augmentation method \cite{li2022domain}

    % transferability of features, Automated synthetic-to-real generalization: \cite{chen2020automated}

\section{Methodology}
    \label{sec:method_dgtta}
    As shown in Fig. \ref{fig:method}, we split our method into two parts: Firstly, the model training takes place on the source domain data by minimizing cross-entropy and Dice loss terms \cite{isensee2021nnu}. We propose doing this with the DG techniques described below to prepare the model for prediction in out-of-domain scenarios. Secondly, our TTA strategy is employed on individual target domain samples after model training.
    % We want to stress, that applying \emph{DG-TTA} on non-DG pre-trained models is also possible, but for optimal results models should be pre-trained with DG techniques.
    \begin{figure*}
        \centerline{\includegraphics[width=\linewidth]{\dgttaPath/figures/method.pdf}}
        \caption{Left: Model pre-training with source domain data. We propose to use \emph{GIN}, \emph{MIND} or their combination \emph{GIN+MIND} in this step. Right: \emph{DG-TTA} method applied in the target data domain. Two differently augmented versions of the same input are passed through the pre-trained segmentation network. The network weights are then optimized supervising the predictions with a Dice loss steering the network to produce consistent predictions.}
        \label{fig:method}
    \end{figure*}

    \subsection{DG pre-training}
    Pre-training is performed on the labeled source training dataset $D_{train}=\{\mathbf{x_s},\mathbf{y_s}\}_{s=1}^{l}$, $l\in\mathbb{N}^+$, where $\mathbf{x_s}$ and $\mathbf{y_s}$ can also be patches.
    Recently, global intensity non-linear augmentation \emph{GIN} \cite{ouyang2022causality} was introduced to improve model generalization during training.
    In \emph{GIN}, a shallow convolutional network with LeakyReLU nonlinearities $g$ is re-initialized at each iteration by random parameters $\rho$ and used to augment the input $\mathbf{x}$.
    The augmented image is then blended with the original image weighted by $\alpha$:
    \begin{equation}
        \text{GIN}(\mathbf{x}) = \alpha\,g_\rho(\mathbf{x}) + (1-\alpha)\,\mathbf{x}
    \end{equation}

    % While training with \emph{GIN} benefits from the random variability of intensity augmentations encountered during,
    We propose using \emph{MIND} descriptors \cite{heinrich2013towards} %TODO: add heinrich2012mind
    as a promising alternative to \emph{GIN}. In contrast to \emph{GIN}, it provides handcrafted features from the input to derive a modality- and contrast-independent representation:
    \begin{equation}
        \text{MIND}(\mathbf{x,p,d}) = \text{exp}\left(-\frac{SSD(\mathbf{x}, \mathbf{p},\mathbf{d})}{\sigma^2_\mathcal{N}}\right),\quad \mathbf{p,d} \in \mathcal{N}
    \end{equation}
    For a given image/patch $\mathbf{x}$, sub-blocks (small patches) at location $\mathbf{p}$ are extracted, and their sum of squared distances (SSD) to neighboring blocks at distance $\mathbf{d}$ are evaluated. This difference is weighted by a local variance estimate $\sigma_\mathcal{N}$, more specifically, the mean of all block differences according to the method.
    The neighborhood $\mathcal{N}$ is defined as all diagonally adjacent blocks of the 6-neighborhood around the center block at $p$ resulting in a mapping of $\mathbb{R}^{1\times\lvert\Omega\rvert} \to \mathbb{R}^{12\times\lvert\Omega\rvert}$ for voxel space $\Omega$. Note that for inference or testing, the \emph{MIND} descriptor is always applied to provide the correct number of input channels to the network.
    For DG, we also propose using the combination of both methods, where before applying the \emph{MIND} descriptor, the input is augmented using \emph{GIN} (\emph{MIND+GIN}).

    \subsection{TTA}
    Our test-time adaptation method can now be applied to pre-trained models: For any given pre-trained model $f_\theta$ on the training dataset $D_{train}$, we want to adjust the weights optimally to a single unseen sample of the target test set $D_{test}=\{\mathbf{x_t}\}$ during TTA.

    Instead of adding additional supervision tasks and complex architectures, we propose to use two augmentations, $A$ and $B$, to obtain differently augmented images. The core idea of the method is to optimize the network to produce consistent predictions given two differently augmented inputs, where
    % $I_{A/B}$ each denote image intensity augmentation and
    $S_{A/B}$ each denote spatial augmentation:
    % \begin{alignat}{2}
    %     \mathbf{x_{A,t}} &= A(\mathbf{x_t}),\quad & A = S_A \circ I_A &\quad A: \mathbb{R}^{\lvert\Omega\rvert} \rightarrow \mathbb{R}^{\lvert\Omega\rvert} \\
    %     \mathbf{x_{B,t}} &= B(\mathbf{x_t}),\quad & B = S_B \circ I_B &\quad B: \mathbb{R}^{\lvert\Omega\rvert} \rightarrow \mathbb{R}^{\lvert\Omega\rvert}
    % \end{alignat}
    \begin{alignat}{2}
        \mathbf{x_{A,t}} &= A(\mathbf{x_t}),\quad & A = S_A &\quad A: \mathbb{R}^{\lvert\Omega\rvert} \rightarrow \mathbb{R}^{\lvert\Omega\rvert} \\
        \mathbf{x_{B,t}} &= B(\mathbf{x_t}),\quad & B = S_B &\quad B: \mathbb{R}^{\lvert\Omega\rvert} \rightarrow \mathbb{R}^{\lvert\Omega\rvert}
    \end{alignat}
    Both augmented images are passed through the pre-trained network $f_\theta$:
    \begin{equation}
        \mathbf{\hat{y}_{A/B,t}} = f_\theta(\mathbf{x_{A/B,t}})
    \end{equation}
    Prior to calculating the consistency loss, both predictions $\mathbf{\hat{y}_{A/B,t}}$ need to be mapped back to the initial spatial orientation for voxel-wise compatibility by applying the inverse transformation operation $S_{A/B}^{-1}$. In addition, a consistency masking $m_{c}(\cdot)$ is applied to filter inversion artifacts with $\zeta$ indicating voxels that were introduced at the image borders during the inverse spatial transformation but are unrelated to the original image content:
    \begin{align}
        m_c(\mathbf{\hat{y}_A}, \mathbf{\hat{y}_B}) &= \left[\mathbf{\hat{y}_A} \neq \zeta\right] \wedge \left[\mathbf{\hat{y}_B}  \neq \zeta\right]\\
        A^{-1} &= m_c \circ S_{A}^{-1} \\
        B^{-1} &= m_c \circ S_{B}^{-1}
    \end{align}
    \columnfigure{\dgttaPath/figures/loss_landscape.pdf}{.8}{Dice loss landscapes and values for $d=[1,2]$ in Eq. \ref{eq:losstta}. $d=2$ yields zero loss along the diagonal, which is favorable for consistency.}{fig:loss}
    We steer the network to produce consistent outputs by comparing them after inversion and masking:
    \begin{equation}
        \mathcal{L}_{TTA} = \ell\left(A^{-1} \circ f_\theta\left(\mathbf{x_{A,t}}\right), B^{-1} \circ f_\theta\left(\mathbf{x_{B,t}}\right)\right)
    \end{equation}
    As loss function $\ell$, we choose a Dice loss with predictions $\mathbf{\hat{y}_A}$ and $\mathbf{\hat{y}_B}$ given as class probabilities for all voxels in $\Omega$:
    \begin{equation}
        \ell(\mathbf{\hat{y}_A}, \mathbf{\hat{y}_B}) = \frac{1}{\lvert\Omega\rvert}\sum_{\omega}^{\Omega} \frac{2 \cdot \hat{y}_{A,\omega} \cdot \hat{y}_{B,\omega}}{{\hat{y}_{A,\omega}}^d + {\hat{y}_{B,\omega}}^d}
        \label{eq:losstta}
    \end{equation}
    Selecting $d=2$ ensures consistency in the Dice loss landscape instead of $d=1$, which forces the network to additionally maximize the confidence of the prediction (see Fig. \ref{fig:loss}).
    % Our method is depicted in Fig. \ref{fig:method} showing two augmentation branches $A$ and $B$, intensity and spatial augmentation blocks, the pre-trained U-Net to be optimized and the consistency masking block prior to loss calculation.
    % During test-time adaptation it can be used with $m=1$ as well as $m>1$ test samples.

    \subsection{Spatial augmentation}
    For spatial augmentation, we use affine image distortions on image/patch coordinates $\mathbf{c_x}$:
    % or deformable
    % $\phi$ for any given spatial coordinate $\mathbf{c_x}$:
    % https://pdf.sciencedirectassets.com/287693/3-s2.0-C20120030396/3-s2.0-B9780124077416000025/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEBcaCXVzLWVhc3QtMSJGMEQCIEtK8pj1cz4x%2FAoEB7c7MZn6VDcZbS33G2u%2FIVXrhjQ%2BAiBAJiLnjWQ%2BuD5JMdWeuDw%2BzTIQDctEFyALZxI5ziolFCqzBQhAEAUaDDA1OTAwMzU0Njg2NSIMfNIQGRmQHvBjjnb%2FKpAFeOb6DT1N%2F65NVbbKXOZaUow%2FrY7aDYatffqklVafakup4H7vYH4vnnIVc6ItgDrmK2yg4fqNUQ40N%2FucdGt102Tau5G41ix7YYa%2B1ww6eaLX3Zk%2BJyjM1%2FLUqTulFSQBNkzxen0voEc5A41yh%2BsCoB2f9z2RAK%2FA%2B25C1eJC6T74sZJOkrCHb%2BTrE5jq6HDLY2%2B%2FUtlwziULwIg0f9La9eGbDDIseAWKDNKrQVM%2BMsU%2BN%2BXdqVGSNzPVcFMLoWPNHJLWHc9G%2BryTepVtCcWFvjFn%2FEsA%2F1sCn7CD1n1tyGYhCs%2FGOzaBnGBDaIgJsEXcjhv%2B4nFQPKzr78gPUSD08BquCRMtnGRFlkgom5HtXJQp5bU7hHmYiRcE%2Bg4HnauXaO5AJvkfh6LT4s8Ob2JL%2FoDf%2FuS6OlVUoP4ZwvhtaOILagjh71nq8sYydTCgvY2y2yZkoLqLZ6PR%2BFDrj7jz6GnI3FlZyc4Es20mAUwLDSRpF3upcFK9hgb2hkKwLI3JglxIXonB5MYouCAv0Gw1%2FAXMo7OJmpEYTdmexiGNLOXgeSVgSym5XxUtglrzeDHCpA9YJEQqYH%2FXuN14uGdY17NXt1KgExPyybfUn9H%2FtmCUrWBi7mtd6jNOweWlxk5ox%2F4Xmsjs%2B7y2lt2XF7MTbXcS6Io7qS9x%2B%2FLxmpJGFHnJWsLyf%2BI4a2QcVUlqTAWAoei9eJu7xwtissdtgRchyRaI02H6Z9hJFwg434QxVje0kMdAnIl2nQHWl6VUVIt4Np1558xGRJUFUSSZ6wkq5kN65dQMJ9FMf84P5p%2FlD02tS6irkqeIL5EYcsrKYkg1l17C5zlpuPPlO9zKxA0YczqUHdzow%2B9wCfGP1%2F33r%2FAw4tTdqQY6sgFfJrD47RzcNgGDZova0l3WxJppqD%2B189%2Fvr4tT0Z5lOT2EOAkwAzqNXVbWuN13MWgs6Fa0e5lPEX64YxLtgcy31v5jax%2Ff%2BvuIzyf%2FzjKXOci5L3Q5g%2F0pgXZ86AgVieHzRVb7qAEaGH8%2FFF5xx2CVqT%2FJaCQwHsh2oUQO2kwlGNflpjJc9k3hzYS6AesjnbYPR5Ph%2FUwPnVavpctPPB2uHFSfcOHM3xRZRSibSY52Z3Un&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20231024T084215Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYVZHHRJOK%2F20231024%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=600c1ad92265a62e71189dcc0144796a5b01c9c4c6ce714888a6ddccad0de794&hash=5ce2fdb469fc0a51d0ae4c6bcd4ada3fd2a46e334c6788e015f4b9bcdf10103a&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=B9780124077416000025&tid=spdf-c93d441c-3719-4bda-bd3e-745cace20caa&sid=64cc2bde66736045669ae6c-e270201d4ee0gxrqb&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=1e005c520d5007040b&rr=81b0ece80e7eca89&cc=de
    % p. 17
    \begin{equation}
        \phi(\mathbf{c_x}) = \mathbf{Mc_x}, \quad
        % \left( \begin{array}{ *{4}{c} }
        %     & & & a_{41} \\
        %     & & & a_{42} \\
        %     \multicolumn{3}{c}
        %       {\raisebox{\dimexpr\normalbaselineskip+.7\ht\strutbox-.5\height}[0pt][0pt]
        %         {\scalebox{2}{$R$}}} & a_{43} \\
        %     0 & 0 & 0 & 1
        %   \end{array} \right)
            \mathbf{M} = \begin{pmatrix}
                r_{11} & r_{12} & r_{13} & t_{14}\\
                r_{21} & r_{22} & r_{23} & t_{24}\\
                r_{31} & r_{32} & r_{33} & t_{34}\\
                0 & 0 & 0 & 1
            \end{pmatrix}
    \end{equation}
    The submatrix $\mathbf{R}$ contains only rotational components
    $r_{11} \dots r_{33}$ such that $\mathbf{R}\mathbf{R^T} = \mathbb{1}$. For the inverse transformation $\mathbf{M}^{-1}$ is used.
    % Deformable augmentation is performed with a B-Spline transformation grid $u$:
    % \begin{equation}
    %     \phi(\mathbf{c_x}) = \mathbf{c_x} + u(\mathbf{c_x})
    % \end{equation}
    % For our experiments we chose $u$ to have $(l_x/4)^3$ control points with $l_x$ being the image/patch size shifted by random distances. We use scaling and squaring presented in \cite{higham2005scaling} to get an invertible deformation field.
    % Preliminary experiments also explored using B-spline distortions, but  no significant improvements over affine distortions were found.
    Although we describe only affine spatial augmentation in this section and Fig. \ref{fig:method}, we also explored B-spline spatial- and intensity augmentation in various combinations during TTA. However, we found affine spatial augmentation on its own to be most effective.


    \subsection{Optimization strategy}
        During TTA, only the classes of interest are optimized, meaning that we omit all other classes of the predictions $\mathbf{\hat{y}_A}$ and $\mathbf{\hat{y}_B}$ for supervision: e.g. in Experiment III in the abdominal TTA scenario, all abdominal classes are supervised, but we omit cardiac, spine, lung and other irrelevant classes from the TS base model (see Sec. \ref{sec:TS_dataset}).
        To increase the robustness of predicted labels, we use an ensemble of three TTA models in the final inference routine of the nnUNet framework \cite{isensee2021nnu}.
        All models were optimized with the \emph{AdamW} optimizer
        % \cite{loshchilov2017decoupled}  %TODO readd again
        with a learning rate of  $\eta=1e{-5}$, weight decay $\beta=0.01$, and no scheduling for twelve epochs.
        In former experiments, we identified that special caution has to be taken when applying test-time adaptation to models that require patch-based input. Since patch-based inference limits the field of view, the optimizer will adapt the model weights and overfit for consistency of the specific image region. Therefore, we accumulate gradients of $N_p = 16$ randomly drawn patches during one epoch prior to taking an optimization step to fit GPU memory limits.

\section{Experiments}
    % With our experimental setup, we want to first emphasize the need for DG/TTA methods in general and then show the potential of TTA.
    Experiment I: We perform an extensive evaluation of models with and without DG capabilities predicting across the domain gap between \fromto{CT}{MR} and \fromto{MR}{CT} and report the associated gaps in accuracy compared to models that have been trained in the target domain.
    Experiment II: TTA is then applied for \fromto{CT}{MR} prediction to show the potential of recovering model performance. We compare our method to a subset of related TTA approaches (see Sec. \ref{sec:related_work}).
    % and the augmentation configuration of different $I_{A/B}$ and $S_{A/B}$ combinations is evaluated to find an optimal setting.
    % We furthermore compare our method to a subset of the presented methods in Sec. \ref{sec:related_work} (Experiment II).
    Experiment III: We provide results on various out-of-domain segmentation scenarios to further broaden our study.

    \subsection{Datasets}
        % The used datasets in this study are describe shortly in the next paragraphs.
        \subsubsection{BTCV: Multi-Atlas Labeling Beyond the Cranial Vault}
        \label{sec:BTCV_dataset}
        The dataset \cite{landman2015miccai} contains 30 labeled abdominal CT scans of a colorectal cancer chemotherapy trial with 14  organs: Spleen (SPL), right kidney (RKN), left kidney (LKN), gallbladder (GAL), esophagus (ESO), liver (LIV), stomach (STO), aorta (AO), inferior vena cava (IVC), portal vein and splenic vein (PSV), pancreas (PAN), right adrenal gland (RAG) and left adrenal gland (LAG).
        Data dimensions reach from \sivoxtuple{512}{512}{85} to \sivoxtuple{512}{512}{198} and fields of view from \simmtuple{280}{280}{280} to \simmtuple{500}{500}{650}. We split the dataset into a 20/10 training/test set for our experiments and use a subset of ten classes that are uniformly labeled in all scans.

        \subsubsection{AMOS: A Large-Scale Abdominal Multi-Organ Benchmark
        for Versatile Medical Image Segmentation}
            \label{sec:amos_dataset}
            The AMOS dataset \cite{ji2022amos} consists of CT and MRI scans from eight scanners with a similar field of view as the BTCV dataset of patients with structural abnormalities in the abdominal region (tumors, etc.). Unlike the BTCV dataset's organs, AMOS has additional segmentation labels for the duodenum, bladder, and prostate/uterus but not for the PSV class. We used 40 of the labeled images (MR only, axial resolution \sinum{0.82} up to \simm{3.00}) and split them into 30/10 training/testing samples so that the number of test samples is comparable to the BTCV dataset.


        \subsubsection{MMWHS: Multi-Modality Whole Heart Segmentation}
            \label{sec:MMWHS_dataset}
            This dataset \cite{zhuang2019evaluation} contains CT and MR images of seven cardiac structures: Left ventricle, right ventricle, left atrium, right atrium, the myocardium of left ventricle, ascending aorta, and pulmonary artery.
            % The field of view captures the whole heart.
            The CT data resolution is \simmpervoxtuple{0.78}{0.78}{0.78}.
            The cardiac MRI data was obtained from two sites with a 1.5 T scanner and reconstructed to obtain resolutions from \simmpervoxtuple{0.80}{0.80}{1.00} down to \simmpervoxtuple{1.00}{1.00}{1.60}.
            We split the labeled 20 CT and 20 MR images into 16/4 training/testing cases.

        \subsubsection{SPINE: MyoSegmenTUM spine}
            \label{sec:SPINE_dataset}
            This MRI dataset \cite{burian2019lumbar} contains water, fat, and proton density-weighted lumbal spine scans with manually labeled vertebral bodies L1 --- L5.
            The field of view spans \simmtuple{220}{220}{80} with a resolution of \simmpervoxtuple{1.8}{1.8}{4.0}.

        \subsubsection{TS: TotalSegmentator, 104 labels}
            \label{sec:TS_dataset}
            The large-scale TS dataset contains CT images of 1204 subjects with 104 annotated classes. The annotations were created semi-automated, where a clinician checked every annotation. The data was acquired across eight sites on 16 scanners with varying slice thickness and resolution  \cite{wasserthal2023totalsegmentator}.
            Differing from the SPINE dataset, the vertebral bodies and the spinous processes are included in the class labels of this dataset, and intermediate model predictions were corrected accordingly in a postprocessing step to obtain reasonable results for evaluation.

        \subsubsection{Pre-/postprocessing}
             We resampled all datasets to a uniform voxel size of \simmpervoxtuple{1.50}{1.50}{1.50}. For the SPINE task, we cropped the TS ground truth to omit the spinous processes with a mask dilated five voxels around the proposed prediction in the \fromto{TS}{SPINE} out-of-domain prediction setting to provide comparable annotations (see Sec. \ref{sec:exp3_description}).

    \subsection{Experiment structure}
        \subsubsection{Experiment I: Domain gap evaluation of non-DG and DG pre-trained models}
            \label{sec:exp1_description}
            The two abdominal datasets, AMOS (MR) and BTCV (CT), are used in this experiment to evaluate out-of-domain gaps of multiple models. More specifically, we evaluate the performance of eight different segmentation models (one 2D model and seven 3D models), all trained in the unified nnUNet framework \cite{isensee2021nnu}. Three models were pre-trained without explicit domain generalization techniques: A 2D nnUNet model (NNUNET 2D) to show the difference of 2D and 3D performance, a 3D nnUNet model (NNUNET) and a 3D nnUNet model with Batch Normalization layers (NNUNET BN) instead of Instance Normalization layers\footnote{Instance Normalization layers are used per default in all models apart from the NNUNET BN model.}.
            % \cite{ioffe2015batch,ulyanov2016instance}. %TODO: readd again
            Five models were pre-trained with domain generalization techniques targeting the input data augmentation: A nnUNet model with extensive spatial and intensity augmentation (INSANE DA), checkerboard masked images with a dropout rate of 30\% (MIC, \cite{hoyer2023mic}), % TODO use CUTOUT instead of MIC?
            \emph{GIN} intensity augmentation (see Sec. \ref{sec:method_dgtta} and  \cite{ouyang2022causality,xu2020robust}), the \emph{MIND} descriptor (see Sec. \ref{sec:method_dgtta} and \cite{heinrich2013towards}) and a combination of \emph{GIN} and \emph{MIND} (\emph{GIN+MIND}). All sub-experiments were conducted in two ways: Training on the CT data and testing on the MR data \fromto{CT}{MR} and vice-versa \fromto{MR}{CT}.

        \subsubsection{Experiment II: Performance evaluation of \emph{DG-TTA}}
            \label{sec:exp2_description}
            This experiment aims to show the potential of our TTA approach subsequently applied to pre-trained models.
            % This experiment aims to show that closing the domain gap in the previous experiment by applying TTA is possible.
            % We selected the best three models with DG capabilities from experiment I as well two non-DG pre-trained models to show applicability to various models:
            % Firstly, we evaluated and optimized the performance of our approach by varying the building blocks of \emph{DG-TTA}:
            % \begin{itemize}
            %     \item Using gradients in the first branch A or in both branches (AB).
            %     \item Enabling GIN intensity augmentation in branch A, branch B, both branches (AB) or not in any branch (/).
            %     \item Enabling spatial augmentation in branch A, branch B, both branches (AB) or not in any branch (/).
            %     \item Using affine or deformable spatial augmentation (affine, deformable) in case spatial augmentation was enabled in any of the branches.
            % \end{itemize}
            % Secondly,
            We evaluate the performance of our and two competing TTA approaches \emph{AdaMI} and \emph{SDA-Net} (see Sec. \ref{sec:related_work}), selecting the competing approaches that share an essential benefit with our proposed \emph{DG-TTA} method that no additional complex generative supervision tasks are necessary.
            For \emph{AdaMI}, a class ratio prior needs to be provided, which we estimate by averaging class voxel counts of the training dataset while we consider the same field of view of the drawn TTA patch.

        \subsubsection{Experiment III: High quality predictions on unseen data out-of-the-box: More scenarios}
            \label{sec:exp3_description}
            Building upon Experiment II, we show efficacy of our method leveraging the TS dataset with 104 labels as a strong basis in three segmentation tasks (all \fromto{CT}{MR}): Abdominal organ-(\fromto{TS}{AMOS}), lumbal spine- (\fromto{TS}{SPINE}) and whole-heart segmentation (\fromto{TS}{MMWHS MR}).
            Furthermore, we present results for whole-heart segmentation using only models pre-trained with as few as 16 CT samples in \fromto{MMWHS CT}{MMWHS MR}.

\section{Results}
    In the following sections, segmentation quality is evaluated using the Dice score overlap metric. The significance of TTA improvements is determined with the one-sided Wilcoxon Signed Rank test \cite{wilcoxon1992individual}.

    \subsection{Experiment I: Domain gap evaluation of non-DG and DG pre-trained models}
    \label{sec:exp1_results}
    \columnfigure{\dgttaPath/figures/exp1.pdf}{1.}{Evaluation of non-DG and DG pre-trained models in out-of-domain prediction scenarios. Dice scores reported of the in-domain prediction (upper baseline, -\,-\,-) and model performance in either \mbox{\fromto{MR}{CT} \legendbox{hi_4}} or \mbox{\fromto{CT}{MR} \legendbox{hi_2}}  out-of-domain scenarios. Detailed gap scores of \mbox{\legendbox{hi_2}-colored} bars see Tab. \ref{tab:exp1}}{fig:exp1}

    \begin{table*}
    \centering
        \caption{Evaluation of models in \fromto{CT}{MR} out-of-domain prediction for abdominal segmentation tasks (pre-training models on the BTCV dataset and testing on the AMOS dataset). This table shows detailed results \mbox{\legendbox{hi_2}-colored} bars of Fig. \ref{fig:exp1}. Full class names can be found in Sec. \ref{sec:BTCV_dataset}}
        \label{tab:exp1}
        \setlength{\tabcolsep}{3pt}
        \resizebox{\textwidth}{!}{
            \begin{tabular}{@{}lS[table-figures-decimal=1,table-format=2.1]lS[table-figures-decimal=1,table-format=2.1]S[table-figures-decimal=1,table-format=2.1]S[table-figures-decimal=1,table-format=2.1]S[table-figures-decimal=1,table-format=2.1]S[table-figures-decimal=1,table-format=2.1]S[table-figures-decimal=1,table-format=2.1]S[table-figures-decimal=1,table-format=2.1]S[table-figures-decimal=1,table-format=2.1]S[table-figures-decimal=1,table-format=2.1]S[table-figures-decimal=1,table-format=2.1]S[table-figures-decimal=1,separate-uncertainty=true,table-format=2.1(3)]S[table-figures-decimal=1,table-format=3.1]@{}}
                \toprule
                  & \multicolumn{1}{l}{\textbf{MRI ID}} & \multicolumn{13}{c}{\textbf{MRI OOD (Dice \%)}}                        \\
                \textbf{Pre-trained model} & \boldmath{$\mu$} & \hspace{5pt} &
                \textbf{\legendbox{map10_10} SPL} &
                \textbf{\legendbox{map10_9} RKN} &
                \textbf{\legendbox{map10_8} LKN} &
                \textbf{\legendbox{map10_7} GAL} &
                \textbf{\legendbox{map10_6} ESO} &
                \textbf{\legendbox{map10_5} LIV} &
                \textbf{\legendbox{map10_4} STO} &
                \textbf{\legendbox{map10_3} AOR} &
                \textbf{\legendbox{map10_2} IVC} &
                \textbf{\legendbox{map10_1} PAN} &
                \boldmath{$\mu\pm\sigma$} &
                \textbf{gap} \\ \midrule
                NNUNET 2D        & 78.8           & & 14.1          & 3.0  & 2.0  & 2.0  & 4.5  & 15.2 & 0.1  & 10.1 & 2.8  & 0.8           & 5.5 \pm 11.2  & -73.3   \\ \midrule
                NNUNET           & 84.5           & & 40.2          & 21.9 & 15.9 & 24.9 & 22.9 & 76.0 & 34.3 & 26.4 & 21.8 & 35.3          & 32.0 \pm 27.6 & -52.6 \\ \hdashline\noalign{\vskip 0.5ex}
                NNUNET BN       & 84.3  & & 0.0           & 0.0  & 0.0  & 0.0  & 0.3  & 43.7 & 0.0  & 2.3  & 0.5  & 1.5           & 4.9 \pm 15.6  & -79.4 \\
                INSANE DA & 84.6  & & 45.3          & 64.0 & 49.1 & 20.2 & 27.3 & 71.0 & 43.0 & 44.4 & 22.8 & 40.4          & 42.7 \pm 30.7 & -40.0 \\
                MIC         & 84.2  & & 0.6           & 0.3  & 1.1  & 2.4  & 3.0  & 23.5 & 0.0  & 9.6  & 6.0  & 3.5           & 5.0 \pm 11.5  & -79.2 \\
                GIN              & 84.8           & & 81.9          & 90.4 & 91.9 & 63.7 & 47.8 & 92.7 & 73.2 & 80.9 & 72.1 & 68.1          & 76.3 \pm 21.5 & -8.5  \\
                MIND             & \B 85.4           & & \B 83.6 & 92.9 & 93.0 & 60.5 & 39.1 & 93.1 & 74.6 & 81.1 & 69.5 & \B 73.2 & 76.1 \pm 24.0 & -9.3  \\
                GIN + MIND &
                84.1 & &
                83.0 &
                \textbf{93.3} &
                \textbf{93.3} &
                \textbf{65.9} &
                \textbf{50.3} &
                \textbf{94.1} &
                \textbf{76.5} &
                \textbf{83.9} &
                \textbf{73.7} &
                71.9 &
                \B 78.6 \pm 20.7 &
                \B -5.5
            \end{tabular}
        }
    \end{table*}
    Prediction across the \fromto{CT}{MR} domain gap in the abdominal task of this experiment reveals that models behave differently depending on the model layer configuration and pre-training strategy (see Fig. \ref{fig:exp1}): As expected, the 2D model achieves lower in-domain scores compared to 3D models.
    For seven of eight models, the domain gap from \fromto{CT}{MR} is more challenging to compensate for, resulting in lower scores during out-of-domain testing. We focus on this domain gap in the following subexperiments and discussions.
    Inference of pre-trained models without generalization techniques results in severe gaps of \sipct{-52.6} and \sipct{-79.4} Dice when predicting in the MRI target domain (see Fig. \ref{fig:exp1} and Tab. \ref{tab:exp1}).
    Increasing the data augmentation strength of the framework (\emph{INSANE DA}) can reduce the gap to \sipct{-40.0} Dice.
    Using techniques specifically designed for DG proves to be more effective: MIC \cite{hoyer2023mic} reduces the gap in the \fromto{MR}{CT} scenario successfully, but we found it to be not as effective for the \fromto{CT}{MR} scenario. \emph{GIN} augmentation \cite{ouyang2022causality} and the \emph{MIND} descriptor layer \cite{heinrich2013towards} %TODO: add heinrich2012mind
    significantly reduce the gap to (\sinum{-8.5}/\sipct{-9.3} Dice).
    Combining \emph{GIN+MIND} results is the best strategy diminishing the gap to only \sipct{-5.5} Dice.
    In detail, using the \emph{GIN+MIND} combination resulted in the highest scores for segmentation in eight of ten abdominal organs (see Tab. \ref{tab:exp1}).

    \subsection{Experiment II: Performance evaluation of \emph{DG-TTA}}
        \label{sec:exp2_tatta_results}
        \begin{figure*}
            \centerline{\includegraphics[width=\linewidth]{\dgttaPath/figures/exp2_tta.pdf}}
            \caption{Test sample statistics before and after applying TTA (three of five models were pre-trained with DG techniques). For the \emph{NNUNET BN} model, additionally, only the Batch Normalisation layers (nor) or only the encoder (enc) parameters were adapted. Results for three of five \emph{SDA-Net} runs excluded due to collapsing predictions. Scores for individual classes can be found in Tab. \ref{tab:exp2_evaluation} for \emph{DG-TTA}, \emph{AdaMI} and \emph{SDA-Net}. \bplotdescription}
            \label{fig:exp2_tta}
        \end{figure*}

        \begin{table*}
            \centering
            \caption{Evaluation of TTA methods for \fromto{CT}{MR} test-time adaptation for the abdominal segmentation task (see Sec. \ref{sec:exp1_description} and Tab. \ref{tab:exp1}). The class names are in Sec. \ref{sec:BTCV_dataset}. Bold scores indicate a per-model increase in scores, taking the scores of the non-adapted models of Tab. \ref{tab:exp1} into account. Results for three of five \emph{SDA-Net} runs excluded due to collapsing predictions.}
            \label{tab:exp2_evaluation}
            \setlength{\tabcolsep}{3pt}
            \resizebox{\textwidth}{!}{
                \begin{tabular}{@{}lllS[table-figures-decimal=1,table-format=2.1]S[table-figures-decimal=1,table-format=2.1]S[table-figures-decimal=1,table-format=2.1]S[table-figures-decimal=1,table-format=2.1]S[table-figures-decimal=1,table-format=2.1]S[table-figures-decimal=1,table-format=2.1]S[table-figures-decimal=1,table-format=2.1]S[table-figures-decimal=1,table-format=2.1]S[table-figures-decimal=1,table-format=2.1]S[table-figures-decimal=1,table-format=2.1]S[table-figures-decimal=1,separate-uncertainty=true,table-format=2.1(3)]S[table-figures-decimal=1,table-format=3.1,explicit-sign=+]@{}}
                    \toprule
                    % \textbf{\textsc{Experiment II}} & & \multicolumn{12}{c}{\textbf{CT \fromto{}{} MR TTA (Dice \%)}} \\
                    \textbf{Pre-trained model} & \textbf{TTA method} & &
                    \textbf{\legendbox{map10_10} SPL} &
                    \textbf{\legendbox{map10_9} RKN} &
                    \textbf{\legendbox{map10_8} LKN} &
                    \textbf{\legendbox{map10_7} GAL} &
                    \textbf{\legendbox{map10_6} ESO} &
                    \textbf{\legendbox{map10_5} LIV} &
                    \textbf{\legendbox{map10_4} STO} &
                    \textbf{\legendbox{map10_3} AOR} &
                    \textbf{\legendbox{map10_2} IVC} &
                    \textbf{\legendbox{map10_1} PAN} &
                    \boldmath{$\mu\pm\sigma$} &
                    \textbf{gain} \\ \midrule
                    \multirow{3}{*}{NNUNET} &  Ours & & \B 76.0 & 70.0 & 74.4 & \B 42.5 & \B 42.0 & 79.8 & 52.0 & 65.2 & 46.7 & 60.8 & 60.9 \pm 25 & 28.9 \\
                    & AdaMI & & 75.0 & \B 77.7 & \B 83.1 & 37.1 & 37.9 & \B 83.8 & \B 56.5 & \B 68.1 & \B 48.2 & \B 61.1 & \B 62.8 \pm 25.3 & \B 30.9 \\
                    & SDA-Net & & 0.0	& 8.1	& 3.6	& 7.9	& 0.3	& 43.1 & 2.5	& 4.8	& 1.4	& 9.3	& 8.2 \pm 17.1	& -23.8 \\ \hdashline\noalign{\vskip 0.5ex}
                    \multirow{2}{*}{NNUNET BN} & Ours & & 81.6 & 87.2 & 89.2 & \B 40.1 & \B 35.9 & 84.7 & \B 73.4 & \B 75.1 & \B 54.1 & \B 66.8 & \B 68.8 \pm 26.6 & \B 63.9 \\
                    & AdaMI & & \B 88.3 & \B 88.3 & \B 90.5 & 34.2 & 30.9 & \B 94.0 & 72.5 & 68.3 & 45.1 & 63.6 & 67.4 \pm 27.9 & 62.4 \\ \hdashline\noalign{\vskip 0.5ex}
                    % & SDA-Net & & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.4 & 0.0 & 0.0 & 0.0 & 0.0	& 0.0 \pm 0.3 & -4.9 \\ \hdashline\noalign{\vskip 0.5ex}
                    \multirow{3}{*}{GIN} & Ours & & 81.6 & \B 90.5 & \B 92.1 & \B 72.3 & \B 48.9 & \B 93.3 & \B 74.7 & 79.2 & 70.9 & \B 70.6 & \B 77.4 \pm 19.8 & \B 1.1 \\
                    & AdaMI & & 78.5 & 86.6 & 90.9 & 71.1 & 47.2 & 92.2 & 69.0 & 78.3 & 68.4 & 66.9 & 74.9 \pm 20.0 & -1.4 \\
                    & SDA-Net & & 70.9 & 86.8 & 86.9 & 60.6 & 41.6 & 89.4 & 69.3 & 74.2 & 65.6 & 54.4 & 70.0 \pm 24.4 & -6.3 \\ \hdashline\noalign{\vskip 0.5ex}
                    \multirow{2}{*}{MIND} & Ours & & 83.4 & 91.2 & 92.9 & 68.1 & \B 48.6 & 91.4 & 74.4 & \B 83.3 & \B 71.3 & 72.2 & \B 77.7 \pm 21.4 & \B 1.6 \\
                    & AdaMI & & 81.1 & 92.4 & 92.5 & \B 69.7 & 41.7 & 92.7 & 72.6 & 80.7 & 69.5 & 72.9 & 76.6 \pm 21.7 & 0.5 \\ \hdashline\noalign{\vskip 0.5ex}
                    % & SDA-Net & & 0.0 & 0.0 & 0.0 & 0.4 & 0.0 & 0.0 & 0.0 & 0.2 & 0.0 & 0.0 & 0.1 \pm 0.4 & -76.0 \\ \hdashline\noalign{\vskip 0.5ex}
                    \multirow{2}{*}{GIN+MIND} & Ours & & 82.2 & 92.7 & 92.7 & 68.4 & 47.1 & 93.4 & 74.5 & \B 84.8 & 73.5 & \B 72.5 & 78.2 \pm 20.7 & -0.4 \\
                    & AdaMI & & 79.4 & 91.5 & 92.6 & \B 70.5 & 47.9 & 91.9 & 73.5 & 80.7 & 72.3 & 70.8 & 77.1 \pm 20 & -1.5 \\
                    % & SDA-Net & & 2.0 & 1.3 & 0.1 & 6.5 & 0.0 & 20.2 & 1.1 & 6.8 & 9.7 & 6.1 & 5.4 \pm 10.5 & -73.2
                \end{tabular}
            }
        \end{table*}
        % For the performance evaluation the best ranked block configuration was used for \emph{DG-TTA} (see previous Sec. \ref{sec:exp2_config_results}).

        In preliminary combinatorial experiments, we found \emph{DG-TTA} using gradients and backpropagation only in branch A and affine spatial augmentation in branches A and B to be an optimal tradeoff between non-DG-  and DG pre-trained models. Note that we do not apply intensity augmentation, which proved counterproductive for DG pre-trained models. These settings will be used in the following experiments.

        \emph{DG-TTA} and \emph{AdaMI} can recover prediction scores on non-DG pre-trained models by a large margin with significant improvements ($p<0.001$) as it can be seen in Fig. \ref{fig:exp2_tta}, left and Tab. \ref{tab:exp2_evaluation}: \emph{AdaMI} reaches a top mean Dice of \sipct{62.8} (\sipct{+30.9}) Dice for the pre-trained \emph{NNUNET} model. \emph{DG-TTA} reaches a top mean Dice of \sipct{68.8} (\sipct{+63.9}) for \emph{NNUNET BN}, which is remarkable comparing the scores to the low out-of-domain performance of the non-adapted model (\sipct{4.9}). As a further ablation, we report scores when only training the Batch Normalisation layers of \emph{NNUNET BN} and for only training the encoder and found the best results when training all model parameters during TTA.
        Evaluating DG pre-trained models, \emph{DG-TTA} improves \emph{GIN} (\sipct{+1.1}) and \emph{MIND} (\sipct{+1.6}) model outcomes. Applied to the \emph{GIN+MIND} model, the Dice score decreases slightly (\sipct{-0.4}). \emph{AdaMI} improves outcomes in the case of the \emph{MIND} pre-trained model (\sipct{+0.5}) but does not reach the performance of our proposed method. We can show the superiority of our approach over the competing methods for four of five adapted models. The tested TTA methods for DG pre-trained models did not lead to statistically significant improvements, given that outcomes were already quite comparable to the in-domain performance.

        Despite thorough finetuning and valid decreasing epoch loss values during TTA, we could not successfully apply \emph{SDA-Net} in this 3D scenario and found scores to be collapsing by a large margin for most of the pre-trained models.

    \subsection{Experiment III: High quality predictions on unseen data out-of-the-box: More scenarios}
        \label{sec:exp3_results}

        \begin{figure*}
            \centering
            \centerline{\includegraphics[width=\linewidth]{\dgttaPath/figures/exp3_visual.pdf}}
            \caption{Visual results correspond to statistics of Fig. \ref{fig:exp3_ts_amos_spine_bplots} and \ref{fig:exp3_bplots_mmwhs}. Rows show source and target task scenarios; columns indicate the base models used. Ground truth is given in the left column. Positively predicted voxels are shown in colors. The erroneous area of predictions is marked with contours. Class colors for the abdominal task can be found in Tab. \ref{tab:exp1}. Whole-heart class labels comprise right ventricle \legendbox{map5_1}, right atrium \legendbox{map5_2}, left ventricle \legendbox{map5_3}, left atrium \legendbox{map5_4} and myocardium \legendbox{map5_5}. Best viewed digitally.}
            \label{fig:exp3_visual}
        \end{figure*}

        \begin{figure*}
        \centering
            \begin{minipage}[t]{.49\linewidth}
                \centering
                \includegraphics[width=\linewidth]{\dgttaPath/figures/exp3_bplots_ts_amos_spine.pdf}
                \caption{Test sample statistics before and after applying \emph{DG-TTA}. Top: Abdominal organ prediction task of Exp. II. Bottom: Lumbal spine tasks. Symbols and axes description see Fig. \ref{fig:exp2_tta}.}
                \label{fig:exp3_ts_amos_spine_bplots}
            \end{minipage}%
            \hfill
            \begin{minipage}[t]{.49\linewidth}
                \centering
                \includegraphics[width=\linewidth]{\dgttaPath/figures/exp3_bplots_mmwhs.pdf}
                \caption{Test sample statistics before and after applying \emph{DG-TTA} for whole heart segmentation tasks. Top: TS pre-trained models. Bottom: MMWHS CT pre-trained models with only 16 training samples. Symbols and axes description see Fig. \ref{fig:exp2_tta}.}
                \label{fig:exp3_bplots_mmwhs}
            \end{minipage}
        \end{figure*}

        Now, we target prediction with \emph{DG-TTA} in various tasks using the TS dataset as a basis since it contains 104 classes, making it a versatile starting point. The test prediction statistics are shown in Fig. \ref{fig:exp3_ts_amos_spine_bplots} and Fig. \ref{fig:exp3_bplots_mmwhs}. Again, DG pre-trained models perform strongly in the AMOS abdominal target task. The highest mean value of \sipct{82.6} Dice can be achieved by applying \emph{DG-TTA} on the \emph{GIN} base model (see Fig. \ref{fig:exp3_ts_amos_spine_bplots}, top). This diminishes the domain gap to only \sipct{-2.2} compared to the MRI in-domain prediction of \sipct{84.8} (see Tab. \ref{tab:exp1}).
        Out-of-domain prediction in the lumbal spine task \fromto{TS}{SPINE} is challenging. Without TTA, only the pre-trained \emph{GIN} model can reach a high performance of \sipct{73.2}. Applying \emph{DG-TTA} on the \emph{GIN+MIND} model leads to recovering the model's capabilities to the highest test score of \sipct{73.7} (\sipct{+15.8}).
        Out-of-domain prediction in the whole-heart segmentation tasks is performed well by DG pre-trained models on the TS dataset (see Fig. \ref{fig:exp3_bplots_mmwhs}, top). The strong \emph{GIN} model performance can be increased to \sipct{82.6} Dice after applying TTA (\sipct{+1.2}).

        With fewer training samples in the \fromto{MMWHS CT}{MR} whole-heart segmentation tasks, lower scores are reported compared to the TS base model (see Fig. \ref{fig:exp3_bplots_mmwhs}, bottom). Applying \emph{DG-TTA} in such a challenging scenario can improve scores for all DG pre-trained models significantly (\sipct{+12.8}, \sipct{+11.8}, \sipct{+11.6}). Here, \emph{GIN+MIND} reaches the best mean of 71.5\,\% Dice.
        Visual results of this experiment are depicted in Fig. \ref{fig:exp3_visual}.

        % Competitors:
        %! SDA-Net https://github.com/heyufan1995/self-domain-adapted-network
        % Li et al.: https://github.com/HaoLi12345/TTA -> No code inside
        %! ADAMI https://github.com/mathilde-b/SFDA

        % RefSeg: NoCode
        % SR-TTT: https://github.com/FeiLyu/SR-TTT -> Not implementable for non-pathologic images?

\section{Discussion and outlook}
    Severe domain shifts in medical imaging currently still prevent the reuse of segmentation models on newly acquired data. It is impossible to estimate \mbox{a priori} whether a pre-trained model with or without DG capabilities functions well out-of-domain. At the time of prediction, the source data might not be available anymore, disclosed due to regulatory limitations, or physically non-transferable. By combining DG and TTA in our proposed \emph{DG-TTA} approach, we can show that prediction across scanner modalities can be improved significantly.
    In case DG works already well on the target domain, subsequent TTA gains are smaller but still yield substantial improvements in most cases. Most of the time, there is no control over how publicly available models are trained; thus, focussing on DG pre-training alone is insufficient. In this regard, improvements of up to \sipct{+63.8} Dice could be achieved for non-DG pre-trained models (see Sec. \ref{sec:exp2_tatta_results}).
    % for the \fromto{BTCV}{AMOS} scenario (\sipct{-0.4} up to \sipct{+1.1} Dice).

    Also, if models assumed to have generalization abilities fail, we can reach significant gains and recover high-quality predictions by applying TTA. This can be seen in the cardiac \fromto{MMWHS CT}{MR} scenario where we report gains from \sipct{+11.6} up to \sipct{+12.8} Dice and in the \fromto{TS}{SPINE} scenario (\sipct{+15.7} up to \sipct{+33.5} Dice).
    Leveraging the TS dataset \cite{wasserthal2023totalsegmentator} together with \emph{DG-TTA}, we provide a powerful tool to obtain high segmentation accuracies on unseen datasets without accessing the source data.
    % even if models were not pre-trained with DG techniques.
    Our method works without prior assumptions regarding the domains involved or class statistics. Compared to related works, our method can easily be integrated into existing pipelines. This is proven by the fact that we provide open source code as a plug-in for the widely known \emph{nnUNet} framework as well as weights for \emph{GIN}, \emph{MIND}, and \emph{GIN+MIND} DG pre-trained TS models ready for out-of-the-box medical image segmentation in demanding scenarios.